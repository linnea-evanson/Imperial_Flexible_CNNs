{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDate: 10 Apr 2020\\n\\nPython version:      3.7\\nPyTorch version:     1.2.0\\n\\n@author: Maksim Lavrov\\n\\nCIFAR10 dataset\\n\\nVGG16 model with one Flexible Layer in block1\\n\\nModified by Linnea Evanson \\n09/01/2021\\nUsed correct definition of flexible layer and baseline and calculated average accuracy to PGD attacks.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Date: 10 Apr 2020\n",
    "\n",
    "Python version:      3.7\n",
    "PyTorch version:     1.2.0\n",
    "\n",
    "@author: Maksim Lavrov\n",
    "\n",
    "CIFAR10 dataset\n",
    "\n",
    "VGG16 model with one Flexible Layer in block1\n",
    "\n",
    "Modified by Linnea Evanson \n",
    "09/01/2021\n",
    "Used correct definition of flexible layer and baseline and calculated average accuracy to PGD attacks.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    " #===================================================== Import libraries ================================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ================================================= Flexible Layer ================================================================================\n",
    "\n",
    "    \n",
    "class FlexiLayer(nn.Module): # class FlexiLayer(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        super(FlexiLayer, self).__init__()\n",
    "        \n",
    "        self.t_1 = nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
    "        self.t_2 = nn.MaxPool2d(self.kernel_size, self.stride, self.padding) # get max result with the same kernel size\n",
    "        self.m = nn.Sigmoid()\n",
    "        \n",
    "        self.threshold1 = Variable(torch.randn((1, self.out_channels, 30, 30)))\n",
    "        \n",
    "        self.thresh_mean = []\n",
    "        \n",
    "    def forward(self, t):\n",
    "        \n",
    "#         self.threshold1.expand(t.size(0), self.out_channels, 30, 30)\n",
    "        \n",
    "        \n",
    "#         cond = torch.sub(self.t_2(t), self.threshold1.cuda())\n",
    "#         t_2_2 = self.m(cond*50)*self.t_2(t) # \n",
    "#         t_1_1 = self.m(cond*(-50))*self.t_1(t) # \n",
    "#         t = torch.add(t_2_2, t_1_1)\n",
    "        \n",
    "        return self.t_1(t)\n",
    "\n",
    "    # ================================================= VGG-16 Network ================================================================================\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16,self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "                      nn.Conv2d(in_channels = 3,out_channels = 64,kernel_size = 3,padding = 1),\n",
    "                      nn.BatchNorm2d(64),\n",
    "                      nn.ReLU(),\n",
    "                      FlexiLayer(in_channels = 64,out_channels = 64,kernel_size = 3, padding =0),\n",
    "                      nn.BatchNorm2d(64),\n",
    "                      nn.ReLU(),\n",
    "                      #nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                      nn.Dropout2d(0.3))\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "                      nn.Conv2d(in_channels = 64,out_channels = 128,kernel_size = 3,padding = 1),\n",
    "                      nn.BatchNorm2d(128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels = 128,out_channels = 128,kernel_size = 3, padding =1),\n",
    "                      nn.BatchNorm2d(128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                      nn.Dropout2d(0.4))\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "                      nn.Conv2d(in_channels = 128,out_channels = 256,kernel_size = 3,padding = 1),\n",
    "                      nn.BatchNorm2d(256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels = 256,out_channels = 256,kernel_size = 3,padding = 1),\n",
    "                      nn.BatchNorm2d(256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels = 256,out_channels = 256,kernel_size = 3, padding =1),\n",
    "                      nn.BatchNorm2d(256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                      nn.Dropout2d(0.4))\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "                      nn.Conv2d(in_channels = 256,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                      nn.BatchNorm2d(512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                      nn.BatchNorm2d(512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3, padding =1),\n",
    "                      nn.BatchNorm2d(512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool2d(kernel_size=2, stride=2) ,\n",
    "                      nn.Dropout2d(0.4))\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "                      nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                      nn.BatchNorm2d(512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                      nn.BatchNorm2d(512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3, padding =1),\n",
    "                      nn.BatchNorm2d(512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                      nn.Dropout2d(0.5) )\n",
    "\n",
    "        self.fc =     nn.Sequential(\n",
    "                      nn.Linear(512,100),\n",
    "                      nn.Dropout(0.5),\n",
    "                      nn.BatchNorm1d(100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.5),\n",
    "                      nn.Linear(100,10), )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = self.block5(out)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from models.conv2model import *\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR PGD Attack Evaluation')\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
    "                    help='input batch size for testing (default: 200)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--epsilon', default=0.031,\n",
    "                    help='perturbation')\n",
    "parser.add_argument('--num-steps', default=20,\n",
    "                    help='perturb number of steps')\n",
    "parser.add_argument('--step-size', default=0.003,\n",
    "                    help='perturb step size')\n",
    "parser.add_argument('--random',\n",
    "                    default=True,\n",
    "                    help='random initialization for PGD')\n",
    "parser.add_argument('--model-path',\n",
    "                    default='./models/VGG16-BASE-nnmodule_withdropoutmodel_150_91.pth',\n",
    "                    help='model for white-box attack evaluation')\n",
    "parser.add_argument('--source-model-path',\n",
    "                    default='./models/conv2_model_80_74.pth',\n",
    "                    help='source model for black-box attack evaluation')\n",
    "parser.add_argument('--target-model-path',\n",
    "                    default='./models/VGG16-BASE-nnmodule_withdropoutmodel_150_91.pth',\n",
    "                    help='target model for black-box attack evaluation')\n",
    "parser.add_argument('--white-box-attack', default=True,\n",
    "                    help='whether perform white-box attack')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# settings\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# set up data loader\n",
    "mean_cifar10 = [0.485, 0.456, 0.406]   \n",
    "std_cifar10 = [0.229, 0.224, 0.225]\n",
    "batch_size = 1\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10,std_cifar10),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../FlexibleCNNs/data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "def _pgd_whitebox(model,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=args.epsilon,\n",
    "                  num_steps=args.num_steps,\n",
    "                  step_size=args.step_size):\n",
    "    out = model(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "    if args.random:\n",
    "        random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(device)\n",
    "        X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "        loss.backward()\n",
    "        eta = step_size * X_pgd.grad.data.sign()\n",
    "        X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "        eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "        X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "    err_pgd = (model(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    #print('err pgd (white-box): ', err_pgd.item())\n",
    "    return err, err_pgd\n",
    "\n",
    "\n",
    "def _pgd_blackbox(model_target,\n",
    "                  model_source,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=args.epsilon,\n",
    "                  num_steps=args.num_steps,\n",
    "                  step_size=args.step_size):\n",
    "    out = model_target(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "    if args.random:\n",
    "        random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(device)\n",
    "        X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model_source(X_pgd), y)\n",
    "        loss.backward()\n",
    "        eta = step_size * X_pgd.grad.data.sign()\n",
    "        X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "        eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "        X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "\n",
    "    err_pgd = (model_target(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    #print('err pgd black-box: ', err_pgd.item())\n",
    "    return err, err_pgd\n",
    "\n",
    "\n",
    "def eval_adv_test_whitebox(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    evaluate model by white-box attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust = _pgd_whitebox(model, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "    print('natural_err_total: ', natural_err_total)\n",
    "    print('robust_err_total: ', robust_err_total)\n",
    "    \n",
    "    return natural_err_total, robust_err_total\n",
    "\n",
    "\n",
    "def eval_adv_test_blackbox(model_target, model_source, device, test_loader):\n",
    "    \"\"\"\n",
    "    evaluate model by black-box attack\n",
    "    \"\"\"\n",
    "    model_target.eval()\n",
    "    model_source.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust = _pgd_blackbox(model_target, model_source, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "    print('natural_err_total: ', natural_err_total)\n",
    "    print('robust_err_total: ', robust_err_total)\n",
    "    \n",
    "    return natural_err_total, robust_err_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgd white-box attack: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d61846391c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mnat_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrob_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_adv_test_whitebox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpgd_naterror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnat_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4b503d5961a1>\u001b[0m in \u001b[0;36meval_adv_test_whitebox\u001b[0;34m(model, device, test_loader)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# pgd attack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0merr_natural\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_robust\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pgd_whitebox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mrobust_err_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merr_robust\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mnatural_err_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merr_natural\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4b503d5961a1>\u001b[0m in \u001b[0;36m_pgd_whitebox\u001b[0;34m(model, X, y, epsilon, num_steps, step_size)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_pgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mX_pgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv_clone/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv_clone/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main\n",
    "pgd_naterror = []\n",
    "pgd_roberror = []\n",
    "\n",
    "for i in range(100): #do 100 iterations then find average robustness\n",
    "    if args.white_box_attack:\n",
    "        # white-box attack\n",
    "        print('pgd white-box attack:', i)\n",
    "        model = VGG16().to(device)\n",
    "        # Load the pretrained model\n",
    "        state = torch.load('./models/VGG16-BASE-nnmodule_withdropoutmodel_150_91.pth')\n",
    "        model.load_state_dict(state['model'])\n",
    "\n",
    "        nat_err, rob_err = eval_adv_test_whitebox(model, device, test_loader)\n",
    "        \n",
    "        pgd_naterror.append(nat_err)\n",
    "        pgd_roberror.append(rob_err)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # black-box attack\n",
    "        print('pgd black-box attack:', i)\n",
    "        model_target = VGG16().to(device)\n",
    "        state_target = torch.load(args.target_model_path)\n",
    "        model_target.load_state_dict(state_target['model'])\n",
    "        model_source = conv2model().to(device)\n",
    "        state_source = torch.load(args.source_model_path)\n",
    "        model_source.load_state_dict(state_source['model'])\n",
    "\n",
    "        nat_err, rob_err = eval_adv_test_blackbox(model_target, model_source, device, test_loader)\n",
    "        pgd_naterror.append(nat_err)\n",
    "        pgd_roberror.append(rob_err)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average Accuracy\n",
    "# Accuracy = 10,000 - error.\n",
    "#It is 10,000 because batch size is 100 images and each image is a %, so 100x100 = 10,000\n",
    "\n",
    "avg_naterror = sum(pgd_naterror)/ len(pgd_naterror)\n",
    "avg_natacc = (10000 - avg_naterror)/100\n",
    "print(\"Average Nat. Acc = \", avg_natacc)\n",
    "\n",
    "avg_roberror = sum(pgd_roberror)/ len(pgd_roberror)\n",
    "avg_robacc = (10000 - avg_roberror)/100\n",
    "print(\"Average Rob. Acc = \", avg_robacc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save\n",
    "# import pickle\n",
    "# with open(\"pgd_whitebox_base_avgrobacc.pkl\", \"wb\") as output_file:\n",
    "#         pickle.dump(avg_robacc, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from the identical .py file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Average Nat. Acc =  tensor(91.4200, device='cuda:0')\n",
    "Average Rob. Acc =  tensor(24.8072, device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7517., device='cuda:0')\n",
    "pgd white-box attack: 87\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7510., device='cuda:0')\n",
    "pgd white-box attack: 88\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7522., device='cuda:0')\n",
    "pgd white-box attack: 89\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7517., device='cuda:0')\n",
    "pgd white-box attack: 90\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7526., device='cuda:0')\n",
    "pgd white-box attack: 91\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7528., device='cuda:0')\n",
    "pgd white-box attack: 92\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7511., device='cuda:0')\n",
    "pgd white-box attack: 93\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7531., device='cuda:0')\n",
    "pgd white-box attack: 94\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7521., device='cuda:0')\n",
    "pgd white-box attack: 95\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7523., device='cuda:0')\n",
    "pgd white-box attack: 96\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7521., device='cuda:0')\n",
    "pgd white-box attack: 97\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7515., device='cuda:0')\n",
    "pgd white-box attack: 98\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7521., device='cuda:0')\n",
    "pgd white-box attack: 99\n",
    "natural_err_total:  tensor(858., device='cuda:0')\n",
    "robust_err_total:  tensor(7528., device='cuda:0')\n",
    "Average Nat. Acc =  tensor(91.4200, device='cuda:0')\n",
    "Average Rob. Acc =  tensor(24.8072, device='cuda:0')\n",
    "\n",
    "PGD_whitebox_attack_VGG16_base_AVERAGED.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
